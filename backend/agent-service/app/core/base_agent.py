"""
Base Agent Interface.

Defines the abstract base class for all CXO domain agents.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel


class AgentDomain(str, Enum):
    """Agent domain types."""
    
    CEO = "ceo"
    HR = "hr"
    FINANCE = "finance"
    OPS = "ops"
    SALES = "sales"
    PROCUREMENT = "procurement"
    LEGAL = "legal"
    IT_SECURITY = "it_security"
    CUSTOMER_SUCCESS = "customer_success"
    PRODUCT = "product"
    CUSTOM = "custom"


class RiskTier(str, Enum):
    """Risk tiers for proposals and actions."""
    
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"


@dataclass
class EvidenceReference:
    """Reference to evidence supporting a claim."""
    
    connector_id: str
    snapshot_id: str
    table_name: str
    row_id: Optional[str]
    excerpt: str
    document_hash: str
    embedding_id: Optional[str] = None
    relevance_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentContext:
    """Context passed to agents for operations."""
    
    tenant_id: str
    user_id: Optional[str] = None
    proposal_id: Optional[str] = None
    round_number: int = 1
    query: Optional[str] = None
    evidence: List[EvidenceReference] = field(default_factory=list)
    memory_context: Dict[str, Any] = field(default_factory=dict)
    config: Dict[str, Any] = field(default_factory=dict)


class ProposalPayload(BaseModel):
    """Proposal generated by an agent."""
    
    title: str
    description: str
    domain: str
    risk_tier: RiskTier
    confidence_score: float
    rationale_bullets: List[str]
    evidence_references: List[Dict[str, Any]]
    payload: Dict[str, Any]
    impact_summary: str
    tradeoffs: List[Dict[str, str]] = []
    counterfactuals: List[str] = []


class ChallengePayload(BaseModel):
    """Challenge raised by an agent."""
    
    challenge_type: str  # data, logic, risk, compliance
    target_id: str
    target_type: str  # proposal, contribution
    content: str
    evidence_references: List[Dict[str, Any]]


class VotePayload(BaseModel):
    """Vote cast by an agent."""
    
    vote: str  # approve, reject, abstain
    rationale: str
    confidence: float
    conditions: List[str] = []


class BaseAgent(ABC):
    """
    Abstract base class for CXO domain agents.
    
    Each agent has:
    - Domain expertise and persona
    - RAG pipeline for evidence retrieval
    - LLM integration for reasoning
    - Memory for learned patterns
    - Execution capabilities
    """
    
    # Class attributes to be defined by subclasses
    domain: AgentDomain = AgentDomain.CUSTOM
    name: str = "Base Agent"
    description: str = "Base agent class"
    
    # Capabilities
    can_read: bool = True
    can_execute: bool = False
    can_propose: bool = True
    can_vote: bool = True
    can_challenge: bool = True
    
    # Default vote weight
    vote_weight: float = 1.0
    
    def __init__(
        self,
        tenant_id: str,
        agent_id: str,
        rag_service: Optional[Any] = None,
        llm_service: Optional[Any] = None,
        memory_service: Optional[Any] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """Initialize the agent with services."""
        self.tenant_id = tenant_id
        self.agent_id = agent_id
        self.rag_service = rag_service
        self.llm_service = llm_service
        self.memory_service = memory_service
        self.config = config or {}
    
    @property
    @abstractmethod
    def system_prompt(self) -> str:
        """Return the agent's system prompt / persona."""
        pass
    
    @property
    @abstractmethod
    def data_sources(self) -> List[str]:
        """Return list of data sources this agent uses."""
        pass
    
    @abstractmethod
    async def generate_proposal(
        self,
        context: AgentContext,
        query: str
    ) -> ProposalPayload:
        """
        Generate a proposal based on the query and context.
        
        Args:
            context: Agent context with tenant, evidence, etc.
            query: The query or topic for the proposal
            
        Returns:
            ProposalPayload with evidence-backed proposal
        """
        pass
    
    @abstractmethod
    async def generate_challenge(
        self,
        context: AgentContext,
        target_proposal: Dict[str, Any]
    ) -> Optional[ChallengePayload]:
        """
        Review a proposal and generate challenges if warranted.
        
        Args:
            context: Agent context
            target_proposal: The proposal to challenge
            
        Returns:
            ChallengePayload if challenge warranted, None otherwise
        """
        pass
    
    @abstractmethod
    async def generate_vote(
        self,
        context: AgentContext,
        proposal: Dict[str, Any],
        deliberation_summary: str
    ) -> VotePayload:
        """
        Cast a vote on a proposal after deliberation.
        
        Args:
            context: Agent context
            proposal: The proposal to vote on
            deliberation_summary: Summary of deliberation rounds
            
        Returns:
            VotePayload with vote decision and rationale
        """
        pass
    
    async def retrieve_evidence(
        self,
        context: AgentContext,
        query: str,
        top_k: int = 5
    ) -> List[EvidenceReference]:
        """
        Retrieve relevant evidence from RAG pipeline.
        
        Args:
            context: Agent context
            query: Query for evidence retrieval
            top_k: Number of results to return
            
        Returns:
            List of evidence references
        """
        if not self.rag_service:
            return []
        
        results = await self.rag_service.search(
            tenant_id=self.tenant_id,
            domain=self.domain.value,
            query=query,
            top_k=top_k
        )
        
        return [
            EvidenceReference(
                connector_id=r.get("connector_id", ""),
                snapshot_id=r.get("snapshot_id", ""),
                table_name=r.get("table_name", ""),
                row_id=r.get("row_id"),
                excerpt=r.get("excerpt", ""),
                document_hash=r.get("document_hash", ""),
                embedding_id=r.get("embedding_id"),
                relevance_score=r.get("relevance_score", 0.0),
                metadata=r.get("metadata", {})
            )
            for r in results
        ]
    
    async def get_memory_context(
        self,
        context: AgentContext,
        memory_types: List[str]
    ) -> Dict[str, Any]:
        """
        Retrieve memory context for the agent.
        
        Args:
            context: Agent context
            memory_types: Types of memory to retrieve
            
        Returns:
            Dictionary of memory contexts
        """
        if not self.memory_service:
            return {}
        
        memories = {}
        for memory_type in memory_types:
            memory = await self.memory_service.get(
                tenant_id=self.tenant_id,
                agent_id=self.agent_id,
                memory_type=memory_type
            )
            if memory:
                memories[memory_type] = memory
        
        return memories
    
    async def save_memory(
        self,
        memory_type: str,
        memory_key: str,
        content: Dict[str, Any]
    ):
        """
        Save memory for the agent.
        
        Args:
            memory_type: Type of memory
            memory_key: Key for retrieval
            content: Memory content
        """
        if not self.memory_service:
            return
        
        await self.memory_service.save(
            tenant_id=self.tenant_id,
            agent_id=self.agent_id,
            memory_type=memory_type,
            memory_key=memory_key,
            content=content
        )
    
    def assess_risk_tier(self, action: Dict[str, Any]) -> RiskTier:
        """
        Assess the risk tier of an action.
        
        Override in subclasses for domain-specific risk assessment.
        
        Args:
            action: The action to assess
            
        Returns:
            RiskTier (LOW, MEDIUM, HIGH)
        """
        # Default implementation - override in subclasses
        return RiskTier.MEDIUM
    
    def format_evidence_for_prompt(
        self,
        evidence: List[EvidenceReference]
    ) -> str:
        """Format evidence references for inclusion in prompts."""
        if not evidence:
            return "No relevant evidence found."
        
        lines = ["Relevant Evidence:"]
        for i, e in enumerate(evidence, 1):
            lines.append(
                f"{i}. [{e.table_name}] {e.excerpt[:200]}... "
                f"(relevance: {e.relevance_score:.2f})"
            )
        
        return "\n".join(lines)
